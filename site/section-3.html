<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Where Orchestration Lives — Agent SDKs for the Rest of Us</title>
    <link rel="stylesheet" href="css/style.css">
</head>
<body class="content-page">

    <aside class="sidebar">
        <div class="sidebar-header">
            <span class="mono-label sidebar-header__label">Report</span>
            <h1 class="sidebar-header__title"><a href="index.html">Agent SDKs for the Rest of Us</a></h1>
        </div>
        <nav class="sidebar-nav">
            <a href="section-1.html" class="nav-item">
                <span class="nav-num">01</span>
                <span class="nav-title">Mapping the Terrain</span>
            </a>
            <a href="section-2.html" class="nav-item">
                <span class="nav-num">02</span>
                <span class="nav-title">What's an Agent, Anyway?</span>
            </a>
            <a href="section-3.html" class="nav-item active">
                <span class="nav-num">03</span>
                <span class="nav-title">Where Orchestration Lives</span>
            </a>
            <a href="section-4.html" class="nav-item">
                <span class="nav-num">04</span>
                <span class="nav-title">Two Tools to Rule Them All</span>
            </a>
            <a href="section-5.html" class="nav-item">
                <span class="nav-num">05</span>
                <span class="nav-title">Agent SDK to Agent Server</span>
            </a>
            <a href="section-6.html" class="nav-item">
                <span class="nav-num">06</span>
                <span class="nav-title">Architecture by Example</span>
            </a>
            <a href="section-7.html" class="nav-item">
                <span class="nav-num">07</span>
                <span class="nav-title">Further Reading</span>
            </a>
        </nav>
    </aside>

    <main class="content-main">
        <div class="top-bar">
            <div class="top-bar__meta">
                <div class="meta-item">
                    <span class="mono-label">Subject</span>
                    <span class="meta-item__value">Agent Frameworks</span>
                </div>
                <div class="meta-item">
                    <span class="mono-label">Date</span>
                    <span class="meta-item__value">Feb 2026</span>
                </div>
            </div>
            <div class="meta-item">
                <span class="mono-label">Section</span>
                <span class="meta-item__value">03 / 07</span>
            </div>
        </div>

        <div class="content-body">
            <div class="content-body__section-num">03</div>
            <h1 class="content-body__title">Where Orchestration Lives</h1>

<p>In part 2 we examine the difference between the <em>Orchestration frameworks</em> and the <em>Agent SDKs</em>. The limit between the 2 can be tenuous, especially with the orchestration frameworks venturing into the Agent SDK&#39;s space.</p>
<p>Who is responsible for the orchestration? That&#39;s where the difference lives. </p>
<h2>How we got here</h2>
<!-- TODO: illustration — progression from single prompt → prompt chain → workflow with tools → agent loop. A horizontal timeline or staircase showing the progression. Reference: the "historical ladder" from Anthropic's "Building Effective Agents" blog post. -->

<p>1️⃣ <strong>At first, people would build one massive prompt and submit it to the LLM.</strong><br>They would cram all the instructions, context, examples, and output format into a single call and hope the LLM would get it right in one pass. Using the restaurant example from Part 1:</p>
<blockquote>
<p>&quot;You are a restaurant assistant. When the user asks for a restaurant, search the web for options near the specified location, then look up reviews for each result, then check availability for the best-rated one, then write a friendly recommendation with available time slots. Format your response as a short paragraph. The user says: Italian food near 123 Main St for Friday evening, party of 4.&quot;</p>
</blockquote>
<p>Everything in one shot: the task description, the steps, the formatting, the input.</p>
<p><strong>This was brittle:</strong></p>
<ul>
<li>LLMs were unreliable on tasks that require multiple steps or intermediate reasoning.</li>
<li>Long prompts produced less predictable output: some parts of the prompt would get overlooked or confuse the model. The longer the prompt, the less consistent the results over multiple runs.</li>
<li>Long prompts were easy to break: even small changes could alter dramatically the behaviour.</li>
</ul>
<p>This made massive prompts hard to fix and improve.</p>
<p>2️⃣ <strong>Getting better results meant breaking things down.</strong><br>Instead of one monolithic prompt, you split the task into smaller steps — each with its own prompt, its own expected output, and its own validation logic. The output of step 1 feeds into step 2, and so on.</p>
<p>The restaurant task becomes: </p>
<blockquote>
<p>Step 1 — parse the user request into structured data.<br>Step 2 — search for restaurants.<br>Step 3 — rank by reviews.<br>Step 4 — check availability.<br>Step 5 — format the response. Each step has a focused prompt, and you can fix or improve one step without breaking the others.</p>
</blockquote>
<p><strong>This is prompt chaining</strong>: a sequence of LLM calls where each step has a narrow, well-defined responsibility.</p>
<p>3️⃣ <strong>Then tools enter the picture.</strong><br>Once you add tool calling (Part 1), each step in the chain can now do real work — query a database, search the web, validate data against an API. The chain becomes a <strong>workflow</strong>: a sequence of steps, some of which involve LLM calls, some of which invoke tools, connected by routing logic.</p>
<p>4️⃣ <strong>With better models, another option emerged</strong>: instead of defining the workflow step by step, give the agent tools and a goal, and let it figure out the steps on its own. We are somewhat back to 1️⃣ — one prompt, one call — but with the addition of tool calling and much better (thinking) models. This is agent-driven control flow, and it coexists with workflows rather than replacing them.</p>
<p>Whether you define the workflow yourself (steps 2️⃣ and 3️⃣) or let the agent figure it out (step 4️⃣), someone has to decide the structure — the sequence of actions that leads to the outcome. That&#39;s what orchestration means.</p>
<blockquote>
<p><strong>Orchestration</strong> is the logic that structures the flow: the sequence of steps, the transitions between them, and how the next step is determined.</p>
</blockquote>
<p>This section focuses on the question: <strong>who owns that logic? who owns the control flow?</strong></p>
<ul>
<li><strong>App-driven control flow</strong>: the logic is decided by the developer and &quot;physically constrained&quot; through code.</li>
<li><strong>Agent-driven control flow</strong>: the logic is suggested by the developer and it is left to the LLM / agent to follow the instructions.</li>
</ul>
<h2>App-driven control flow</h2>
<p><strong>Within the app-driven control flow, the app owns the state machine</strong>:</p>
<ul>
<li>The developer defines the graph: the nodes (steps), the edges (transitions), the routing logic.</li>
<li>The LLM is a component called within each step but the app enforces the flow defined by the developer.</li>
</ul>
<!-- TODO: illustration — app-driven restaurant workflow graph: START → Parse Request → Search Restaurants → Get Reviews → Check Availability → Format Response → END. Show the nodes as boxes with arrows indicating the fixed sequence defined by the developer. -->


<p>Anthropic&#39;s <a href="https://www.anthropic.com/research/building-effective-agents">&quot;Building Effective Agents&quot;</a> blog post catalogs several variants of app-driven control flow:</p>
<ul>
<li><strong>Prompt chaining</strong> — each LLM call processes the output of the previous one.</li>
<li><strong>Routing</strong> — an LLM classifies an input and directs it to a specialized follow-up.</li>
<li><strong>Parallelization</strong> — LLMs work simultaneously on subtasks, outputs are aggregated.</li>
<li><strong>Orchestrator-workers</strong> — a central LLM breaks down tasks and delegates to workers.</li>
<li><strong>Evaluator-optimizer</strong> — one LLM generates, another evaluates, in a loop.</li>
</ul>
<p><strong>Orchestration frameworks provide the infrastructure for building these workflows.</strong> They abstract the plumbing so that developers can focus on the workflow logic. More specifically they handle:</p>
<ul>
<li>Parsing tool calls, feeding results back into the next model call.</li>
<li>Stop conditions, error handling, retries, timeouts.</li>
</ul>
<p>Here is schematically how the developer would implement the restaurant reservation workflow:</p>
<pre><code>workflow = new Workflow()

workflow.add_step(&quot;search&quot;,       search_restaurants)
workflow.add_step(&quot;get_reviews&quot;,  fetch_reviews)
workflow.add_step(&quot;check_avail&quot;,  check_availability)
workflow.add_step(&quot;respond&quot;,      format_response)

workflow.add_route(&quot;search&quot;      → &quot;get_reviews&quot;)
workflow.add_route(&quot;get_reviews&quot;  → &quot;check_avail&quot;)
workflow.add_route(&quot;check_avail&quot;  → &quot;respond&quot;)

result = workflow.run(&quot;Italian restaurant near the office, Friday, 4 people&quot;)
</code></pre>
<p>On top of that, the developer defines the functions for each step. For example, <code>search_restaurants</code> might use the LLM internally to parse search results:</p>
<pre><code>function search_restaurants(query, location):
    raw_results = web_search(query + &quot; near &quot; + location)
    parsed = llm(&quot;Extract restaurant names and addresses from: &quot; + raw_results)
    return parsed
</code></pre>
<p>How the main orchestration frameworks compare:</p>
<ul>
<li><strong>LangGraph</strong> (Python + TypeScript) was the first such framework. You wire every node and edge by hand. </li>
<li><strong>PydanticAI</strong> (Python) takes a different approach: graph transitions are defined as return type annotations on nodes, so the type checker enforces valid transitions at write-time. </li>
<li><strong>Vercel AI SDK</strong> (Typescript) started as a low-level tool loop + unified provider layer, then added agent abstractions in v5-v6 (2025). </li>
<li><strong>Mastra</strong> (Typescript) builds on top of Vercel AI SDK — it delegates model routing and tool calling to the AI SDK and adds the application layer on top (workflows, memory, evaluation).</li>
</ul>
<p>There are other such orchestration frameworks. Cues to recognize app-driven control flows:</p>
<ul>
<li>Explicit stage transitions in code or config.</li>
<li>Multiple different prompts or schemas per stage.</li>
<li>The app decides when to request user input.</li>
<li>The model may call tools <em>within</em> a step, but the <strong>macro progression</strong> is app-owned.</li>
</ul>
<h2>Agent-driven control flow</h2>
<p><strong>With Agent-driven control flow, the agent decides what happens next</strong>:</p>
<ul>
<li>The agent is provided with a goal, some context and instructions, and some tools. </li>
<li>The agent decides which tools to call in which order: there is no explicit graph. No developer-defined state machine.</li>
</ul>
<p>It looks like this:</p>
<pre><code>agent = Agent(
    model = &quot;claude-sonnet&quot;,
    system_prompt = &quot;You are a coding assistant. Read files, edit code,
                     run tests. Fix the failing test in src/auth.ts.&quot;,
    tools = [read_file, edit_file, run_tests, search_codebase], 
    max_turns = 50
)

result = agent.run(&quot;Fix the failing test in src/auth.ts&quot;)
</code></pre>
<p>The agent decides:</p>
<ul>
<li>What to read first.</li>
<li>What to edit.</li>
<li>When to run tests.</li>
<li>Whether to try a different approach after a failure.</li>
<li>When to stop.</li>
</ul>
<p><strong>The orchestration moves <em>inside</em> the agent loop</strong>: it&#39;s encoded in the system prompt, the available tools, and the model&#39;s own judgment.</p>
<p>This is the model behind coding agents like Claude Code, Codex, and similar systems. Anthropic renamed their Claude Code SDK to the Claude Agent SDK precisely because this pattern applies beyond coding — they use the same loop for research, video creation, and note-taking.</p>
<p>Typical signs of agent-driven control flow:</p>
<ul>
<li><strong>The hosting app is thin</strong>: it relays messages, enforces permissions, renders results.</li>
<li><strong>The logic lives in the harness</strong> in the form of system prompts, context files, skills and other &quot;capabilities&quot; that steer the agent towards the expected outcome.</li>
</ul>
<h2>The harness</h2>
<p>When there is no developer-defined graph, the harness is what keeps the agent on track. It is the set of assets and capabilities provided to the agent to steer it towards the expected outcome:</p>
<ul>
<li><strong>System prompts, policies and instructions (in agent.md or similar)</strong>: the rules of the road: what to do, what not to do, how to behave.</li>
<li><strong>Tools</strong>: what pre-packaged tools are available to search, fetch, edit, run commands, apply patches.</li>
<li><strong>Permissions</strong>: which tools are allowed, under what conditions, with what scoping.</li>
<li><strong>Skills</strong>: pre-packaged behaviours and assets the agent can invoke.</li>
<li><strong>Hooks / callbacks</strong> — places the host can intercept or augment behavior: logging, approvals, guardrails.</li>
</ul>
<p>This report examines three agent SDKs that implement agent-driven control flow:</p>
<ul>
<li><strong>Claude Agent SDK</strong> exposes the Claude Code engine as a library, with all the harness elements above built in.</li>
<li><strong>OpenCode</strong> ships as a server with an HTTP API — the harness plus a ready-made service boundary (see Part 4).</li>
<li><strong>Pi SDK</strong> is an opinionated, minimalistic framework. Notably it can work in environments without bash or filesystem access, relying on structured tool calls instead.</li>
</ul>
<p>Part 4 examines how these three differ in what they provide and what you need to build yourself.</p>
<p><strong>Note</strong>: Orchestration frameworks are adding modes to create agent-driven control flows:</p>
<ul>
<li>LangChain added &quot;Deep Agents&quot; in July 2025. The <code>deepagents</code> package ships all of this as built-in middleware on top of LangGraph.</li>
<li>PydanticAI lists &quot;Deep Agents&quot; as a first-class multi-agent pattern — planning, filesystem operations, task delegation, sandboxed code execution.</li>
</ul>
<h2>What to keep in mind</h2>
<p>Three points from this section:</p>
<ul>
<li><strong>Orchestration is about who decides what happens next.</strong> In app-driven control flow, the developer defines the graph. In agent-driven control flow, the model decides based on goals, tools, and prompts. Both are valid — the choice depends on how predictable the task is.</li>
<li><strong>Orchestration frameworks handle the plumbing.</strong> Whether you choose app-driven or agent-driven, frameworks give you the loop, tool wiring, and error handling so you can focus on the logic — not on parsing JSON and managing retries.</li>
<li><strong>In agent-driven systems, the harness replaces the graph.</strong> The agent has more freedom, but it is not unsupervised. System prompts, permissions, skills, and hooks are what steer it. The harness is the developer&#39;s control surface when there is no explicit workflow.</li>
</ul>

        </div>
    </main>

</body>
</html>
